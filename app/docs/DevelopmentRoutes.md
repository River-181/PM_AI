# 개발 환경 및 최종 목표 (Development Environment & Final Goal)

이 문서는 '소리-빛 번역 AI' 프로젝트의 기술적 접근 방식과 최종 목표 하드웨어/소프트웨어에 대한 개요를 설명합니다.

---

## 1. 기술 루트 A: Python 기반 R&D 및 모델링

프로젝트 초기 단계에서 고려했던 첫 번째 기술 경로는 **Python**을 중심으로 한 연구개발(R&D) 및 모델링입니다. 이는 AI 모델 개발의 표준적인 접근 방식입니다.

*   **핵심 라이브러리**:
    *   **데이터 처리/분석**: `Librosa`, `NumPy`, `Pandas`를 사용하여 오디오 데이터를 스펙트로그램으로 변환하고, 특성(feature)을 추출하며, 데이터를 정규화합니다.
    *   **모델 훈련**: `TensorFlow (Keras)` 또는 `PyTorch`를 사용하여 CNN, RNN 등 다양한 딥러닝 아키텍처의 사운드 분류 모델을 설계하고 훈련합니다.
*   **개발 프로세스**:
    1.  **데이터셋 구축**: 다양한 환경의 소리 데이터를 수집하고 `Librosa`를 통해 전처리합니다.
    2.  **모델 실험**: Python 환경에서 여러 모델 구조와 하이퍼파라미터를 실험하며 최적의 모델을 탐색합니다.
    3.  **모델 평가**: 훈련된 모델의 정확도, 정밀도, 재현율 등 다양한 지표를 통해 성능을 검증합니다.
*   **한계 및 전환 이유**:
    *   **배포의 어려움**: Python으로 훈련된 모델을 모바일 기기나 임베디드 시스템에 직접 배포하는 것은 복잡합니다. 모델을 `TensorFlow Lite`나 `Core ML` 형식으로 변환하는 과정이 필요하며, 이 과정에서 성능 저하가 발생할 수 있습니다.
    *   **실시간 처리**: 모바일 기기에서 Python 런타임을 통해 실시간으로 오디오 스트림을 처리하는 것은 상당한 성능 부하와 지연(latency)을 유발할 수 있습니다.

> **결론**: Python 기반 접근은 모델의 성능을 극한으로 끌어올리는 **연구 단계**에서는 매우 유용하지만, 최종 목표인 **온디바이스(On-Device) 실시간 AI**를 구현하기에는 비효율적이라고 판단했습니다.

---

## 2. 기술 루트 B: Apple 생태계 기반 온디바이스 AI (채택)

우리가 최종적으로 채택한 경로는 Apple의 네이티브 프레임워크를 최대한 활용하여 **기기 자체에서 모든 AI 연산을 수행**하는 온디바이스(On-Device) 방식입니다.

*   **핵심 프레임워크**:
    *   **모델 추론**: `Core ML`을 사용하여 Python에서 훈련하고 변환된 AI 모델을 iOS 기기에서 직접, 효율적으로 실행합니다. 서버 없이도 AI 기능을 완벽하게 수행할 수 있습니다.
    *   **실시간 오디오 분석**: `SoundAnalysis` 프레임워크를 사용하여 `AVFoundation`에서 들어오는 오디오 스트림을 실시간으로 분석하고, 모델이 예측할 수 있는 형태로 데이터를 공급합니다. 이는 매우 낮은 지연 시간과 높은 효율성을 보장합니다.
    *   **UI 및 상태 관리**: `SwiftUI`를 사용하여 AI 모델의 예측 결과에 따라 UI가 즉각적으로 변화하는 **반응형(Reactive) 인터페이스**를 구축합니다.
*   **개발 프로세스**:
    1.  **모델 변환**: Python으로 훈련된 모델을 `coremltools`를 사용해 `.mlpackage` 형식으로 변환합니다.
    2.  **네이티브 통합**: 변환된 모델을 Xcode 프로젝트에 통합하고, `SoundClassifier` 클래스를 통해 `SoundAnalysis` 요청을 생성합니다.
    3.  **UI 연동**: `SoundClassifier`를 `ObservableObject`로 만들어, 모델의 예측 결과를 `@Published` 프로퍼티로 UI(View)에 전달합니다.
*   **장점**:
    *   **성능 및 효율성**: 모든 연산이 기기 내에서 최적화된状態で実行されるため、非常に高速で効率的です。
    *   **개인정보 보호**: 사용자의 소리 데이터가 외부 서버로 전송되지 않아 개인정보를 완벽하게 보호할 수 있습니다.
    *   **오프라인 작동**: 인터넷 연결 없이도 앱의 모든 핵심 기능이 정상적으로 작동합니다.

> **결론**: Apple 생태계 기반의 온디바이스 AI는 **실시간성, 성능, 개인정보 보호**라는 세 마리 토끼를 모두 잡을 수 있는 최적의 솔루션입니다. 현재 개발 중인 iOS 앱은 이 접근 방식의 **프로토타입**입니다.

---

## 3. 최종 목표: '쿵덕이' 하드웨어 및 연동 소프트웨어

이 프로젝트의 최종 목표는 단순한 앱 출시가 아닌, **독립적인 하드웨어 제품 '쿵덕이'** 와 이를 지원하는 소프트웨어 생태계를 구축하는 것입니다.

*   **쿵덕이 하드웨어 (The Hardware)**:
    *   **형태**: 미니멀한 디자인의 스탠드형 스마트 조명.
    *   **핵심 부품**:
        *   고감도 마이크: 주변 소리를 감지.
        *   저전력 AI 칩 (e.g., NPU가 탑재된 SoC): 온디바이스 AI 모델을 실행하여 소리를 실시간으로 분석.
        *   다채널 RGB LED: 분석된 소리의 종류와 중요도에 따라 다양한 색상과 패턴의 빛을 표현.
        *   Wi-Fi/Bluetooth 모듈: 스마트폰 앱과 연동하여 설정 및 알림을 관리.
    *   **목표**: 사용자의 공간에 자연스럽게 녹아들어, 청각 정보를 시각 정보로 조용히 번역해주는 **앰비언트(Ambient) 정보 기기**.

*   **연동 소프트웨어 (The Software)**:
    *   **임베디드 소프트웨어**: '쿵덕이' 하드웨어 내부에서 실행되는 경량 운영체제 및 AI 추론 엔진.
    *   **스마트폰 앱 (현재 프로토타입)**:
        *   **설정 및 제어**: '쿵덕이' 하드웨어의 민감도 조절, 특정 소리에 대한 알림 여부 설정, LED 밝기 및 색상 테마 커스터마이징.
        *   **이력 및 통계**: 지난 시간 동안 감지된 소리 이벤트의 로그를 확인하고, 특정 소리의 발생 빈도를 통계로 제공.
        *   **원격 알림**: '화재 경보', '아기 울음' 등 중요한 소리가 감지되었을 때, 사용자의 스마트폰으로 푸시 알림을 전송.

> **궁극적 비전**: '쿵덕이'는 단순한 소리 감지기를 넘어, **일상의 소음 속에서 의미 있는 정보를 발견**하고 이를 아름다운 빛으로 표현하여 사용자의 삶의 질을 높이는 **라이프스타일 AI 동반자**가 되는 것을 목표로 합니다.

---

## 4. 사용된 주요 라이브러리 및 프레임워크 (Key Libraries & Frameworks Used)

'소리-빛 번역 AI' 앱은 Apple의 강력한 개발 생태계를 활용하여 구축되었습니다. 주요 사용된 라이브러리 및 프레임워크는 다음과 같습니다.

*   **SwiftUI**: 선언형(Declarative) UI를 통해 사용자 인터페이스를 쉽고 빠르게 구축하는 데 사용되었습니다. AI 모델의 예측 결과에 따라 UI가 실시간으로 반응하도록 하는 핵심적인 역할을 합니다.
*   **AVFoundation**: 마이크로부터 오디오 입력을 처리하고, 오디오 세션을 관리하며, 실시간 오디오 스트림을 `SoundAnalysis` 프레임워크로 전달하는 데 사용됩니다.
*   **SoundAnalysis**: 실시간 오디오 스트림을 분석하고, `Core ML` 모델과 연동하여 소리 분류를 수행하는 데 최적화된 프레임워크입니다. 낮은 지연 시간으로 효율적인 온디바이스 분석을 가능하게 합니다.
*   **Core ML**: Python에서 훈련된 머신러닝 모델(`Koongdeok-soundAI_4.mlpackage`)을 iOS/macOS 앱에서 직접 실행할 수 있도록 하는 프레임워크입니다. 온디바이스 AI의 핵심 기술입니다.
*   **coremltools (Python)**: Python 환경에서 훈련된 딥러닝 모델(예: TensorFlow, PyTorch)을 `Core ML` 형식(`.mlmodel`, `.mlpackage`)으로 변환하는 데 사용되는 Python 라이브러리입니다. (앱 내에서 직접 사용되지는 않지만, 모델 준비 과정에서 필수적으로 사용됨)
*   **UserDefaults**: 앱의 설정 값(예: 각 소리 라벨의 신뢰도 임계값, 기준 데시벨)을 기기에 영구적으로 저장하고 불러오는 데 사용됩니다.
