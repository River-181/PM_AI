
---

## ğŸ¯ **Gemini Prompt**

**System / Instruction:**
You are an expert in **on-device sound classification AI** and **SwiftUI app integration**.
Analyze the provided Core ML model specs, label taxonomy, and app requirements, then propose improvements, UX ideas, and implementation suggestions that maximize accuracy, efficiency, and user experience.

---

### ğŸ§  Model Specification

* **Model Type:** Core ML `AudioClassifier`
* **Frameworks Used:** Create ML (Sound Classification) â†’ CoreML + SoundAnalysis
* **Input:**

  * `audioSamples` â€“ mono audio waveform, 16 kHz, Float32, 15,600 samples (~1 sec)
* **Output:**

  * `target` â€“ predicted class label (String)
  * `targetProbability` â€“ probability dictionary for all classes
* **Performance:**

  * Validation Accuracy: ~80%
  * Test Accuracy: ~77%
* **Hardware Environment:** iOS app (Xcode, SwiftUI), CPU-only inference
* **Model Purpose:** Detect and classify everyday household sounds for contextual feedback.

---

### ğŸ§ Label Set (Korean â†’ Meaning)

| Category          | Label Name                       | Color Mapping |
| ----------------- | -------------------------------- | ------------- |
| ğŸ”´ Fire/Safety    | `ì•ˆì „_í™”ì¬_ê²½ë³´`                       | Red           |
| ğŸŸ  Door Events    | `ë¬¸_ë…¸í¬`, `ë¬¸_ë„ì–´ë½`, `ë¬¸_ì´ˆì¸ì¢…`         | Orange        |
| ğŸŸ¡ Phone & Alerts | `í°_ë§í†¤`, `í°_ì•Œë¦¼`, `í°_ì „í™”`, `íƒ€ì´ë¨¸_ì•ŒëŒ` | Yellow        |
| ğŸŸ¢ Appliances     | `ê°€ì „ì œí’ˆ_ì¢…ë£Œ`                        | Green         |
| ğŸ”µ Baby           | `ì•„ê¸°_ìš¸ìŒ`                          | Blue          |
| ğŸŸ£ Animal/Noise   | `ê°œ`, `ì†ŒìŒ_*`                      | Purple        |
| âš« Ambient         | `ë°°ê²½ì†ŒìŒ`, `ì •ì `                     | Gray          |
| ğŸŸ¦ Fall Detection | `ì•ˆì „_ë‚™ìƒ`                          | Indigo        |

---

### ğŸ“± App Requirements

* **Platform:** iOS (SwiftUI + CoreML + SoundAnalysis)

* **Goal:**
  Display a character (â€œì¿µë•ì´â€) whose color changes dynamically according to the detected sound class.

* **Logic:**

  1. Microphone input analyzed in real-time via `SNAudioStreamAnalyzer`.
  2. Classification result updates every ~1 second.
  3. UI reacts immediately:

     * Color changes according to the label-color mapping above.
     * Label name and confidence displayed.
  4. Background/ambient noise yields neutral gray color.
  5. Designed for eventual integration with hardware (IoT visual indicator).

* **Implementation Constraints:**

  * CPU-only inference (no GPU/ANE access)
  * Background audio processing allowed
  * Lightweight UI for low-latency feedback

---

### ğŸ’¡ What to Generate

Please:

1. Review and summarize the AI workflow (sound â†’ label â†’ color).
2. Suggest improvements to accuracy, real-time stability, and label grouping.
3. Recommend UX/UI enhancements for â€œì¿µë•ì´â€ color feedback.
4. Optionally, propose model architecture or data augmentation strategies for better generalization.

---

### ğŸ” Example Output

You might reply with:

* â€œSuggested model improvements (architecture/data)â€
* â€œOptimized label mapping / threshold tuningâ€
* â€œEnhanced SwiftUI animation and background handlingâ€
* â€œFuture direction: Merging multiple sounds, adaptive color blending, emotion feedback, etc.â€

---

