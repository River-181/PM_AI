# 🎧🔆 소리-빛 번역 AI 모델 개발기 (공모전 발표 스토리라인)

---

## 1. 서론: 문제 제기

- **목표**: 청각 약자를 위해 가정 내에서 발생하는 다양한 생활 소리를 실시간으로 감지하고, 이를 시각 정보(빛)로 번역하여 알려주는 AI 시스템 개발
- **핵심 과제**: 어떻게 다양한 소리를 정확하게 분류하고, 사용자에게 직관적으로 전달할 것인가?

---

## 2. 데이터 준비: 원석을 보석으로

- **데이터 수집**: 화재 경보, 초인종, 아이 울음 등 6가지 주요 생활 소음 데이터를 수집했습니다.
- **데이터 정제 (1차)**: 수작업으로 파일명을 정리하고, `baby_crying`, `door_lock` 등 일관된 기준으로 폴더를 표준화했습니다.
- **데이터 정제 (2차)**: 일부 오디오 파일이 너무 길다는 점을 발견, 5초 단위로 잘게 나누는 'Chunking' 기법을 적용하여 데이터의 양을 늘리고(Augmentation) 길이를 통일했습니다. (`trim_audio.py` 스크립트 활용)
- **최종 구조**: 모델 학습에 용이하도록, 모든 데이터를 `train` (80%) / `val` (20%) 폴더로 나누고, 각 폴더 안에 클래스별로 데이터를 구성했습니다.

```
/data
├── train/
│   ├── fire_alarm/
│   ├── door_event/
│   └── ...
└── val/
    ├── fire_alarm/
    ├── door_event/
    └── ...
```

---

## 3. 데이터 탐색 (EDA): 데이터를 알아야 이긴다

> 이 과정은 `eda_and_model_selection.ipynb` 노트북에 상세히 기록되어 있습니다.

- **클래스 분포 분석**: 데이터를 시각화한 결과, '아이 울음' 데이터가 다른 클래스에 비해 상대적으로 많은 것을 확인했습니다. 이는 특정 소리에 편향될 가능성을 의미하며, 모델 평가 시 주의가 필요합니다.
- **오디오 길이 분석**: 대부분의 소리가 5초 내외로 짧게 발생한다는 인사이트를 얻었습니다. 이는 모델이 긴 시간의 맥락보다 짧은 순간의 특징을 잘 포착해야 함을 시사합니다.
- **스펙트로그램 시각화**: 각 클래스의 소리를 '로그-멜 스펙트로그램'으로 변환하여 시각화했습니다. 그 결과, 클래스별로 뚜렷하게 구분되는 고유한 시각적 패턴을 발견했으며, 이를 통해 이미지 분류 모델을 활용한 접근법의 성공 가능성을 확인할 수 있었습니다.

---

## 4. 모델 선정 실험: 최적의 두뇌를 찾아서

> 이 과정 역시 `eda_and_model_selection.ipynb` 노트북에 상세히 기록되어 있습니다.

- **실험 목표**: 어떤 모델 아키텍처가 우리의 '소리 이미지'를 가장 잘 이해하는지 찾기 위해 성능 비교 실험을 진행했습니다.
- **대결 모델 3가지**:
  1.  **Custom CNN**: 우리가 직접 설계한 간단한 CNN 모델
  2.  **MobileNetV2**: 속도에 중점을 둔 경량 전이학습 모델
  3.  **EfficientNetB0**: 성능과 효율의 균형을 맞춘 강력한 전이학습 모델
- **실험 결과**: 10 Epoch 동안의 짧은 학습 결과, **EfficientNetB0**가 다른 모델들에 비해 월등히 높은 검증 정확도(Validation Accuracy)를 달성했습니다.
- **결론**: 따라서 `EfficientNetB0`를 최종 베이스 모델로 선정하고, 이 모델을 중심으로 추가적인 성능 튜닝을 진행하기로 결정했습니다.

---

## 5. 최종 모델 구현 및 학습

> 전체 구현 코드는 `soundlight_mvp.ipynb` 노트북에서 확인할 수 있습니다.

- **아키텍처**: ImageNet으로 사전 학습된 `EfficientNetB0`를 기반으로, 마지막 단에 우리 데이터의 클래스 수(5개)에 맞는 분류기를 추가했습니다.
- **학습 전략**: 
  - **전이 학습(Transfer Learning)**: 초기에는 사전 학습된 가중치를 고정하고 새로 추가한 분류기만 학습시켜, 모델이 '소리 이미지'의 기본 특징을 빠르게 익히도록 했습니다.
  - **미세 조정(Fine-tuning)**: 이후, 모델의 일부 상위 레이어의 고정을 풀어 아주 작은 학습률(learning rate)로 추가 학습을 진행했습니다. 이를 통해 모델이 우리 데이터의 세부적인 특징에 더 잘 적응하도록 성능을 극대화했습니다.
- **과적합 방지**: `EarlyStopping`, `ModelCheckpoint`, `ReduceLROnPlateau` 등의 콜백 함수를 사용하여 불필요한 학습을 막고 최적의 성능을 내는 지점의 모델을 저장했습니다.

---

## 6. 시연: 소리가 빛이 되는 순간

- **추론 기능**: 학습된 모델은 두 가지 방식으로 작동합니다.
  1.  **파일 기반 추론**: 녹음된 오디오 파일을 입력하면 어떤 소리인지 즉시 분류합니다.
  2.  **실시간 추론**: 마이크를 통해 들어오는 소리를 0.5초 간격으로 계속 분석하여 실시간으로 소리를 감지하고 분류합니다.
- **빛 번역**: 모델이 특정 소리를 감지하면, 미리 정의된 `CLASS_TO_COLOR` 매핑에 따라 지정된 색상 정보를 출력합니다. (예: `fire_alarm` 감지 → `RED` 출력)

---

## 7. 결론 및 향후 과제

- **성과**: 본 프로젝트를 통해, 다양한 생활 소음을 효과적으로 분류하는 AI 모델을 성공적으로 개발했으며, EDA부터 모델 선정, 최종 학습까지의 체계적인 개발 파이프라인을 구축했습니다.
- **향후 과제**:
  1.  **데이터 확장**: 현재 부족한 클래스(가전제품 소리 등)의 데이터를 추가 수집하여 모델의 강건성(robustness)을 높일 필요가 있습니다.
  2.  **모델 경량화**: 실제 임베디드 하드웨어(라즈베리파이 등)에서 원활하게 동작할 수 있도록 모델 경량화(Quantization, Pruning) 연구가 필요합니다.
  3.  **하드웨어 연동**: 개발된 모델을 실제 LED 조명 하드웨어와 연동하여 최종 프로토타입을 완성할 계획입니다.
